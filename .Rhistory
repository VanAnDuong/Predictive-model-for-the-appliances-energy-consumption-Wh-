sprintf("Histogram of Y")
},
xlab = if(i <= 4) {
sprintf("X%d", i) #Name of five features
} else {
sprintf("Y") #Name of Y
})
}
#Save data
new_data <- nor_data
write.table(new_data,"VanAnDuong-transformed.txt")
#Task 3
#Load source package
source("AggWaFit718.R")
#WAM
fit.QAM(new_data[,c(1:4,5)],"WAMoutput.txt","WAMstats.txt")
#WPM p= 0.5
fit.QAM(new_data[,c(1:4,5)], "PMoutput.txt", "PMstats.txt",g=PM05,g.inv = invPM05)
#WPM p= 2
fit.QAM(new_data[,c(1:4,5)],"QMoutput.txt", "QMstats.txt",g=QM,g.inv = invQM)
#OWA
fit.OWA(new_data[,c(1:4,5)],"OWAoutput1.txt","OWAstats1.txt")
#RMSE and Av. abs error of OWA are 0.172272806591901 and 0.126527832140127
#It indicates that the OWA has the Highest accuracy
#Task 4
#Choose OWA as it has a lowest RMSE
weight <- c(0.800547752843707, 0.0895834648215261, 0.109868782334779, 0)
#Assign input and expected output
Y <- 60 #Expected output
X <- c(19.1, 43.29, 19.7, 43.4) #Input
#Transform input
for (i in 1:4) {
if (i == 2) { #If it is X2
X[i] <- (X[i])^3 #Apply third power for it
} else { #Others
X[i] <- log(X[i]) #Apply log transformation for them
}
}
#Min max (use log_data to be same scale with input)
for(i in 1:4) {
X[i] <- (X[i]-min(log_data[,i]))/(max(log_data[,i])-min(log_data[,i]))
}
#Negation X2
X[2] <- 1 - X[2] #1 is a max value from (0,1)
#Use OWA function to predict
Y_pre <- OWA(X,weight) #0.298907807416299
#Transfer predicted Y to the same scale with expected Y
Y_scale = min(log_data[,5]) + (max(log_data[,5]) - min(log_data[,5])) * Y_pre
Y_predicted<-exp(Y_scale) #65.71
#Compare expected Y and predicted Y
percent = ((Y_predicted - Y)/Y)*100
print(percent) #9.520946
#It is a reasonable output because the difference is smaller than 10%
citation(package = "moments")
citation(package = "moments")
citation()
citation(AggWaFit718.R")
citation(AggWaFit718")
citation()
# Use for loop to draw scatter plot from 5 column with the last column
for(i in 1:5) { # 1:5 is the range from the first column to the fifth column
plot(my.data[, i], my.data[, 6],  #Draw scatter plot for each feature with Y
xlab = sprintf("X%d", i),    #Name of features
ylab = "Y",                  #Name of Y
main = sprintf("Scatter Plot of X%d and Y", i)) #Title
}
# Use for loop to draw histograms
for (i in 1:6) {
hist(my.data[, i], #Draw histogram for each column
main = if(i <= 5) { #Title of five features
sprintf("Histogram of X%d", i)
} else { #Title of Y
sprintf("Histogram of Y")
},
xlab = if(i <= 5) {
sprintf("X%d", i) #Name of five features
} else {
sprintf("Y") #Name of Y
})
}
#I can identify a positive skew for X1, negative skew for X2, positive skew for X3, positive skew for X4, negative skew for X5, and positive skew for Y
#Task 2
#It is hard to choose variables, so I will use correlation
correlations <- numeric(5) #define a vector contains 5 results
for (i in 1:5) { #From feature 1 to 5
correlations[i] <- cor(my.data[, i], my.data[, 6]) #Calculate correlations
}
print(correlations) #Print results 0.22691526 -0.09273544  0.09493204  0.06363531 -0.04648270
#As the results above, X1, X2, X3, and X4 has strong relationship, so they are selected to predict.
#Transform data
log_data <- cbind(my.data[,1], my.data[,2], my.data[,3], my.data[,4], my.data[,6])
#The transformation of six columns to reduce skewness
for (i in 1:5) {
if (i == 2) { #If it is X2 (X2 has a negative skew)
log_data[, i] <- (log_data[, i])^3 #Apply third power for it
} else { #Others have a positive skew
log_data[, i] <- log(log_data[, i]) #Apply log transformation for them
}
}
#After transforming data, I use Min-Max normalisation to make them be on the same scale [0,1]
#Min max transformation
nor_data <- log_data
for(i in 1:5) {
nor_data[, i] <- (nor_data[,i]-min(nor_data[,i]))/(max(nor_data[,i])-min(nor_data[,i]))
}
print(cor(nor_data[,2], nor_data[,5])) #-0.09665012
#As X2 has a negative correlation with Y, I apply negation function for it
nor_data[,2] <- 1 - nor_data[,2] #1 is a max value from (0,1)
# Use for loop to draw histograms to check nor_data after transforming
for (i in 1:5) {
hist(nor_data[, i], #Draw histogram for each column
main = if(i <= 4) { #Title of five features
sprintf("Histogram of X%d", i)
} else { #Title of Y
sprintf("Histogram of Y")
},
xlab = if(i <= 4) {
sprintf("X%d", i) #Name of five features
} else {
sprintf("Y") #Name of Y
})
}
#Save data
new_data <- nor_data
write.table(new_data,"VanAnDuong-transformed.txt")
#Task 3
#Load source package
source("AggWaFit718.R")
#WAM
fit.QAM(new_data[,c(1:4,5)],"WAMoutput.txt","WAMstats.txt")
#WPM p= 0.5
fit.QAM(new_data[,c(1:4,5)], "PMoutput.txt", "PMstats.txt",g=PM05,g.inv = invPM05)
#WPM p= 2
fit.QAM(new_data[,c(1:4,5)],"QMoutput.txt", "QMstats.txt",g=QM,g.inv = invQM)
#OWA
fit.OWA(new_data[,c(1:4,5)],"OWAoutput1.txt","OWAstats1.txt")
#RMSE and Av. abs error of OWA are 0.172272806591901 and 0.126527832140127
#It indicates that the OWA has the Highest accuracy
#Task 4
#Choose OWA as it has a lowest RMSE
weight <- c(0.800547752843707, 0.0895834648215261, 0.109868782334779, 0)
#Assign input and expected output
Y <- 60 #Expected output
X <- c(19.1, 43.29, 19.7, 43.4) #Input
#Transform input
for (i in 1:4) {
if (i == 2) { #If it is X2
X[i] <- (X[i])^3 #Apply third power for it
} else { #Others
X[i] <- log(X[i]) #Apply log transformation for them
}
}
#Min max (use log_data to be same scale with input)
for(i in 1:4) {
X[i] <- (X[i]-min(log_data[,i]))/(max(log_data[,i])-min(log_data[,i]))
}
#Negation X2
X[2] <- 1 - X[2] #1 is a max value from (0,1)
#Use OWA function to predict
Y_pre <- OWA(X,weight) #0.298907807416299
#Transfer predicted Y to the same scale with expected Y
Y_scale = min(log_data[,5]) + (max(log_data[,5]) - min(log_data[,5])) * Y_pre
Y_predicted<-exp(Y_scale) #65.71
#Compare expected Y and predicted Y
percent = ((Y_predicted - Y)/Y)*100
print(percent) #9.520946
#It is a reasonable output because the difference is smaller than 10%
#References
#Candanedo, L.M., Feldheim, V. and Deramaix, D., 2017. Data driven prediction models of energy use of appliances in a low-energy house. Energy and buildings, 140, pp.81-97.
citation()
#R Core Team (2023). _R: A Language and Environment for Statistical Computing_. R Foundation forStatistical Computing, Vienna, Austria. <https://www.R-project.org/>.
# Use for loop to draw scatter plot from 5 column with the last column
for(i in 1:5) { # 1:5 is the range from the first column to the fifth column
plot(my.data[, i], my.data[, 6],  #Draw scatter plot for each feature with Y
xlab = sprintf("X%d", i),    #Name of features
ylab = "Y",                  #Name of Y
main = sprintf("Scatter Plot of X%d and Y", i)) #Title
}
# Use for loop to draw histograms
for (i in 1:6) {
hist(my.data[, i], #Draw histogram for each column
main = if(i <= 5) { #Title of five features
sprintf("Histogram of X%d", i)
} else { #Title of Y
sprintf("Histogram of Y")
},
xlab = if(i <= 5) {
sprintf("X%d", i) #Name of five features
} else {
sprintf("Y") #Name of Y
})
}
#I can identify a positive skew for X1, negative skew for X2, positive skew for X3, positive skew for X4, negative skew for X5, and positive skew for Y
#Task 2
#It is hard to choose variables, so I will use correlation
correlations <- numeric(5) #define a vector contains 5 results
for (i in 1:5) { #From feature 1 to 5
correlations[i] <- cor(my.data[, i], my.data[, 6]) #Calculate correlations
}
print(correlations) #Print results 0.22691526 -0.09273544  0.09493204  0.06363531 -0.04648270
#As the results above, X1, X2, X3, and X4 has strong relationship, so they are selected to predict.
#Transform data
log_data <- cbind(my.data[,1], my.data[,2], my.data[,3], my.data[,4], my.data[,6])
#The transformation of six columns to reduce skewness
for (i in 1:5) {
if (i == 2) { #If it is X2 (X2 has a negative skew)
log_data[, i] <- (log_data[, i])^3 #Apply third power for it
} else { #Others have a positive skew
log_data[, i] <- log(log_data[, i]) #Apply log transformation for them
}
}
#After transforming data, I use Min-Max normalisation to make them be on the same scale [0,1]
#Min max transformation
nor_data <- log_data
for(i in 1:5) {
nor_data[, i] <- (nor_data[,i]-min(nor_data[,i]))/(max(nor_data[,i])-min(nor_data[,i]))
}
print(cor(nor_data[,2], nor_data[,5])) #-0.09665012
#As X2 has a negative correlation with Y, I apply negation function for it
nor_data[,2] <- 1 - nor_data[,2] #1 is a max value from (0,1)
# Use for loop to draw histograms to check nor_data after transforming
for (i in 1:5) {
hist(nor_data[, i], #Draw histogram for each column
main = if(i <= 4) { #Title of five features
sprintf("Histogram of X%d", i)
} else { #Title of Y
sprintf("Histogram of Y")
},
xlab = if(i <= 4) {
sprintf("X%d", i) #Name of five features
} else {
sprintf("Y") #Name of Y
})
}
#Save data
new_data <- nor_data
write.table(new_data,"VanAnDuong-transformed.txt")
#Task 3
#Load source package
source("AggWaFit718.R")
#WAM
fit.QAM(new_data[,c(1:4,5)],"WAMoutput.txt","WAMstats.txt")
#WPM p= 0.5
fit.QAM(new_data[,c(1:4,5)], "PMoutput.txt", "PMstats.txt",g=PM05,g.inv = invPM05)
#WPM p= 2
fit.QAM(new_data[,c(1:4,5)],"QMoutput.txt", "QMstats.txt",g=QM,g.inv = invQM)
#OWA
fit.OWA(new_data[,c(1:4,5)],"OWAoutput1.txt","OWAstats1.txt")
#RMSE and Av. abs error of OWA are 0.172272806591901 and 0.126527832140127
#It indicates that the OWA has the Highest accuracy
#The orness is approximate 0.103 (nearly 0) indicates that after sorting, higher weights are applied to the lower values
#Task 4
#Choose OWA as it has a lowest RMSE
weight <- c(0.800547752843707, 0.0895834648215261, 0.109868782334779, 0)
#Assign input and expected output
Y <- 60 #Expected output
X <- c(19.1, 43.29, 19.7, 43.4) #Input
#Transform input
for (i in 1:4) {
if (i == 2) { #If it is X2
X[i] <- (X[i])^3 #Apply third power for it
} else { #Others
X[i] <- log(X[i]) #Apply log transformation for them
}
}
#Min max (use log_data to be same scale with input)
for(i in 1:4) {
X[i] <- (X[i]-min(log_data[,i]))/(max(log_data[,i])-min(log_data[,i]))
}
#Negation X2
X[2] <- 1 - X[2] #1 is a max value from (0,1)
#Use OWA function to predict
Y_pre <- OWA(X,weight) #0.298907807416299
#Transfer predicted Y to the same scale with expected Y
Y_scale = min(log_data[,5]) + (max(log_data[,5]) - min(log_data[,5])) * Y_pre
Y_predicted<-exp(Y_scale) #65.71
#Compare expected Y and predicted Y
percent = ((Y_predicted - Y)/Y)*100
print(percent) #9.520946
#It is a reasonable output because the difference is smaller than 10%
#References
#Candanedo, L.M., Feldheim, V. and Deramaix, D., 2017. Data driven prediction models of energy use of appliances in a low-energy house. Energy and buildings, 140, pp.81-97.
citation()
#R Core Team (2023). _R: A Language and Environment for Statistical Computing_. R Foundation forStatistical Computing, Vienna, Austria. <https://www.R-project.org/>.
# Use for loop to draw scatter plot from 5 column with the last column
for(i in 1:5) { # 1:5 is the range from the first column to the fifth column
plot(my.data[, i], my.data[, 6],  #Draw scatter plot for each feature with Y
xlab = sprintf("X%d", i),    #Name of features
ylab = "Y",                  #Name of Y
main = sprintf("Scatter Plot of X%d and Y", i)) #Title
}
# Use for loop to draw histograms
for (i in 1:6) {
hist(my.data[, i], #Draw histogram for each column
main = if(i <= 5) { #Title of five features
sprintf("Histogram of X%d", i)
} else { #Title of Y
sprintf("Histogram of Y")
},
xlab = if(i <= 5) {
sprintf("X%d", i) #Name of five features
} else {
sprintf("Y") #Name of Y
})
}
#I can identify a positive skew for X1, negative skew for X2, positive skew for X3, positive skew for X4, negative skew for X5, and positive skew for Y
#Task 2
#It is hard to choose variables, so I will use correlation
correlations <- numeric(5) #define a vector contains 5 results
for (i in 1:5) { #From feature 1 to 5
correlations[i] <- cor(my.data[, i], my.data[, 6]) #Calculate correlations
}
print(correlations) #Print results 0.22691526 -0.09273544  0.09493204  0.06363531 -0.04648270
#As the results above, X1, X2, X3, and X4 has strong relationship, so they are selected to predict.
#Transform data
log_data <- cbind(my.data[,1], my.data[,2], my.data[,3], my.data[,4], my.data[,6])
#The transformation of six columns to reduce skewness
for (i in 1:5) {
if (i == 2) { #If it is X2 (X2 has a negative skew)
log_data[, i] <- (log_data[, i])^3 #Apply third power for it
} else { #Others have a positive skew
log_data[, i] <- log(log_data[, i]) #Apply log transformation for them
}
}
#After transforming data, I use Min-Max normalisation to make them be on the same scale [0,1]
#Min max transformation
nor_data <- log_data
for(i in 1:5) {
nor_data[, i] <- (nor_data[,i]-min(nor_data[,i]))/(max(nor_data[,i])-min(nor_data[,i]))
}
print(cor(nor_data[,2], nor_data[,5])) #-0.09665012
#As X2 has a negative correlation with Y, I apply negation function for it
nor_data[,2] <- 1 - nor_data[,2] #1 is a max value from (0,1)
# Use for loop to draw histograms to check nor_data after transforming
for (i in 1:5) {
hist(nor_data[, i], #Draw histogram for each column
main = if(i <= 4) { #Title of five features
sprintf("Histogram of X%d", i)
} else { #Title of Y
sprintf("Histogram of Y")
},
xlab = if(i <= 4) {
sprintf("X%d", i) #Name of five features
} else {
sprintf("Y") #Name of Y
})
}
#Save data
new_data <- nor_data
write.table(new_data,"VanAnDuong-transformed.txt")
#Task 3
#Load source package
source("AggWaFit718.R")
#WAM
fit.QAM(new_data[,c(1:4,5)],"WAMoutput.txt","WAMstats.txt")
#WPM p= 0.5
fit.QAM(new_data[,c(1:4,5)], "PMoutput.txt", "PMstats.txt",g=PM05,g.inv = invPM05)
#WPM p= 2
fit.QAM(new_data[,c(1:4,5)],"QMoutput.txt", "QMstats.txt",g=QM,g.inv = invQM)
#OWA
fit.OWA(new_data[,c(1:4,5)],"OWAoutput1.txt","OWAstats1.txt")
#RMSE and Av. abs error of OWA are 0.172272806591901 and 0.126527832140127
#It indicates that the OWA has the Highest accuracy
#The orness is approximate 0.103 (nearly 0) indicates that after sorting, higher weights are applied to the lower values
#Task 4
#Choose OWA as it has a lowest RMSE
weight <- c(0.800547752843707, 0.0895834648215261, 0.109868782334779, 0)
#Assign input and expected output
Y <- 60 #Expected output
X <- c(19.1, 43.29, 19.7, 43.4) #Input
#Transform input
for (i in 1:4) {
if (i == 2) { #If it is X2
X[i] <- (X[i])^3 #Apply third power for it
} else { #Others
X[i] <- log(X[i]) #Apply log transformation for them
}
}
#Min max (use log_data to be same scale with input)
for(i in 1:4) {
X[i] <- (X[i]-min(log_data[,i]))/(max(log_data[,i])-min(log_data[,i]))
}
#Negation X2
X[2] <- 1 - X[2] #1 is a max value from (0,1)
#Use OWA function to predict
Y_pre <- OWA(X,weight) #0.298907807416299
#Transfer predicted Y to the same scale with expected Y
Y_scale = min(log_data[,5]) + (max(log_data[,5]) - min(log_data[,5])) * Y_pre
Y_predicted<-exp(Y_scale) #65.71
#Compare expected Y and predicted Y
percent = ((Y_predicted - Y)/Y)*100
print(percent) #9.520946
#It is a reasonable output because the difference is smaller than 10%
#References
#Candanedo, L.M., Feldheim, V. and Deramaix, D., 2017. Data driven prediction models of energy use of appliances in a low-energy house. Energy and buildings, 140, pp.81-97.
citation()
#R Core Team (2023). _R: A Language and Environment for Statistical Computing_. R Foundation forStatistical Computing, Vienna, Austria. <https://www.R-project.org/>.
#Choose random 400 records in the.data. In the.data, the first column is id, so it will not be choose.
my.data <- the.data[sample(1:num_samples,num_row), c(2:num_col)]
# Use for loop to draw scatter plot from 5 column with the last column
for(i in 1:5) { # 1:5 is the range from the first column to the fifth column
plot(my.data[, i], my.data[, 6],  #Draw scatter plot for each feature with Y
xlab = sprintf("X%d", i),    #Name of features
ylab = "Y",                  #Name of Y
main = sprintf("Scatter Plot of X%d and Y", i)) #Title
}
# Use for loop to draw histograms
for (i in 1:6) {
hist(my.data[, i], #Draw histogram for each column
main = if(i <= 5) { #Title of five features
sprintf("Histogram of X%d", i)
} else { #Title of Y
sprintf("Histogram of Y")
},
xlab = if(i <= 5) {
sprintf("X%d", i) #Name of five features
} else {
sprintf("Y") #Name of Y
})
}
#I can identify a positive skew for X1, negative skew for X2, positive skew for X3, positive skew for X4, negative skew for X5, and positive skew for Y
#Task 2
#It is hard to choose variables, so I will use correlation
correlations <- numeric(5) #define a vector contains 5 results
for (i in 1:5) { #From feature 1 to 5
correlations[i] <- cor(my.data[, i], my.data[, 6]) #Calculate correlations
}
print(correlations) #Print results 0.22691526 -0.09273544  0.09493204  0.06363531 -0.04648270
#As the results above, X1, X2, X3, and X4 has strong relationship, so they are selected to predict.
#Transform data
log_data <- cbind(my.data[,1], my.data[,2], my.data[,3], my.data[,4], my.data[,6])
#The transformation of six columns to reduce skewness
for (i in 1:5) {
if (i == 2) { #If it is X2 (X2 has a negative skew)
log_data[, i] <- (log_data[, i])^3 #Apply third power for it
} else { #Others have a positive skew
log_data[, i] <- log(log_data[, i]) #Apply log transformation for them
}
}
#After transforming data, I use Min-Max normalisation to make them be on the same scale [0,1]
#Min max transformation
nor_data <- log_data
for(i in 1:5) {
nor_data[, i] <- (nor_data[,i]-min(nor_data[,i]))/(max(nor_data[,i])-min(nor_data[,i]))
}
print(cor(nor_data[,2], nor_data[,5])) #-0.09665012
#As X2 has a negative correlation with Y, I apply negation function for it
nor_data[,2] <- 1 - nor_data[,2] #1 is a max value from (0,1)
# Use for loop to draw histograms to check nor_data after transforming
for (i in 1:5) {
hist(nor_data[, i], #Draw histogram for each column
main = if(i <= 4) { #Title of five features
sprintf("Histogram of X%d", i)
} else { #Title of Y
sprintf("Histogram of Y")
},
xlab = if(i <= 4) {
sprintf("X%d", i) #Name of five features
} else {
sprintf("Y") #Name of Y
})
}
#Save data
new_data <- nor_data
write.table(new_data,"VanAnDuong-transformed.txt")
#Task 3
#Load source package
source("AggWaFit718.R")
#WAM
fit.QAM(new_data[,c(1:4,5)],"WAMoutput.txt","WAMstats.txt")
#WPM p= 0.5
fit.QAM(new_data[,c(1:4,5)], "PMoutput.txt", "PMstats.txt",g=PM05,g.inv = invPM05)
#WPM p= 2
fit.QAM(new_data[,c(1:4,5)],"QMoutput.txt", "QMstats.txt",g=QM,g.inv = invQM)
#OWA
fit.OWA(new_data[,c(1:4,5)],"OWAoutput1.txt","OWAstats1.txt")
#RMSE and Av. abs error of OWA are 0.172272806591901 and 0.126527832140127
#It indicates that the OWA has the Highest accuracy
#The orness is approximate 0.103 (nearly 0) indicates that after sorting, higher weights are applied to the lower values
#Task 4
#Choose OWA as it has a lowest RMSE
weight <- c(0.800547752843707, 0.0895834648215261, 0.109868782334779, 0)
#Assign input and expected output
Y <- 60 #Expected output
X <- c(19.1, 43.29, 19.7, 43.4) #Input
#Transform input
for (i in 1:4) {
if (i == 2) { #If it is X2
X[i] <- (X[i])^3 #Apply third power for it
} else { #Others
X[i] <- log(X[i]) #Apply log transformation for them
}
}
#Min max (use log_data to be same scale with input)
for(i in 1:4) {
X[i] <- (X[i]-min(log_data[,i]))/(max(log_data[,i])-min(log_data[,i]))
}
#Negation X2
X[2] <- 1 - X[2] #1 is a max value from (0,1)
#Use OWA function to predict
Y_pre <- OWA(X,weight) #0.298907807416299
#Transfer predicted Y to the same scale with expected Y
Y_scale = min(log_data[,5]) + (max(log_data[,5]) - min(log_data[,5])) * Y_pre
Y_predicted<-exp(Y_scale) #65.71
#Compare expected Y and predicted Y
percent = ((Y_predicted - Y)/Y)*100
print(percent) #9.520946
#It is a reasonable output because the difference is smaller than 10%
#References
#Candanedo, L.M., Feldheim, V. and Deramaix, D., 2017. Data driven prediction models of energy use of appliances in a low-energy house. Energy and buildings, 140, pp.81-97.
citation()
#R Core Team (2023). _R: A Language and Environment for Statistical Computing_. R Foundation forStatistical Computing, Vienna, Austria. <https://www.R-project.org/>.
